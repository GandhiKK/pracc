{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python387jvsc74a57bd0ee7447c447970e93d9127156525ab891246cc857d123cc4c788834933ed162f5",
   "display_name": "Python 3.8.7 64-bit ('.pracc': venv)",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "ee7447c447970e93d9127156525ab891246cc857d123cc4c788834933ed162f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Обучение в подкреплением в PyBullet (Keras)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "import random, numpy, math, time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "import pybullet as pb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "source": [
    "## Окружение\n",
    "\\reset \\step \\render"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 1000        # максимальное количество шагов симуляции\n",
    "STEPS_AFTER_TARGET = 30 # количество шагов симуляции после достижения цели\n",
    "TARGET_DELTA = 0.2      # величина приемлемого качения возле цели (абсолютное значение)\n",
    "FORCE_DELTA = 0.1       # шаг измениния силы (абсолютное значение)\n",
    "PB_BallMass = 1         # масса шара\n",
    "PB_BallRadius = 0.2     # радиус шара\n",
    "PB_HEIGHT = 10          # максимальная высота поднятия шара\n",
    "MAX_FORCE = 20          # максимальная вертикальная сила пиложенная к шару\n",
    "MIN_FORCE = 0           # минимальнпая сила пиложенная к шару\n",
    "MAX_VEL = 14.2          # максимальная вертикальная скорость шара\n",
    "MIN_VEL = -14.2         # минимальная вертикальная скорость шара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        # текущее состояние окружения\n",
    "        self.pb_z = 0        # текущая высота шара\n",
    "        self.pb_force = 0    # текущая сила приложенная к шару\n",
    "        self.pb_velocity = 0 # текущая вертикальная скорость шара\n",
    "        self.z_target = 0    # целевая высота\n",
    "        self.start_time = 0  # время начала новой игры\n",
    "        self.steps = 0       # количество шагов после начала симуляции\n",
    "        self.target_area = 0        # факт достежения цели\n",
    "        self.steps_after_target = 0 # количество шагов после достежения цели\n",
    "\n",
    "        # создадим симуляцию\n",
    "        self.pb_physicsClient = pb.connect(pb.DIRECT)\n",
    "\n",
    "    def reset(self):\n",
    "        # случайные высота шара и целевая высота\n",
    "        z_target = random.uniform(0.01, 0.99)\n",
    "        self.z_target = PB_BallRadius + z_target*PB_HEIGHT\n",
    "        z = random.uniform(0.05, 0.95)\n",
    "        self.pb_z = PB_BallRadius + z*PB_HEIGHT\n",
    "        \n",
    "        # сброс параметров окружения\n",
    "        pb.resetSimulation()\n",
    "        self.target_area = 0\n",
    "        self.start_time = time.time()\n",
    "        self.steps = 0\n",
    "        self.steps_after_target = 0\n",
    "        \n",
    "        # шаг симуляции 1/60 сек.\n",
    "        pb.setTimeStep(1./60)\n",
    "        \n",
    "        # поверхность\n",
    "        floorColShape = pb.createCollisionShape(pb.GEOM_PLANE)\n",
    "        # для GEOM_PLANE, visualShape - не отображается, будем использовать GEOM_BOX\n",
    "        floorVisualShapeId = pb.createVisualShape(pb.GEOM_BOX,halfExtents=[100,100,0.0001], rgbaColor=[1,1,.98,1])\n",
    "        self.pb_floorId = pb.createMultiBody(0,floorColShape,floorVisualShapeId, [0,0,0], [0,0,0,1])# (mass,collisionShape,visualShape)\n",
    "        \n",
    "        # шар\n",
    "        ballPosition = [0,0,self.pb_z]\n",
    "        ballOrientation=[0,0,0,1]\n",
    "        ballColShape = pb.createCollisionShape(pb.GEOM_SPHERE,radius=PB_BallRadius)\n",
    "        ballVisualShapeId = pb.createVisualShape(pb.GEOM_SPHERE,radius=PB_BallRadius, rgbaColor=[0.25, 0.75, 0.25,1])\n",
    "        self.pb_ballId = pb.createMultiBody(PB_BallMass, ballColShape, ballVisualShapeId, ballPosition, ballOrientation) #(mass, collisionShape, visualShape, ballPosition, ballOrientation)\n",
    "        #pb.changeVisualShape(self.pb_ballId,-1,rgbaColor=[1,0.27,0,1])\n",
    "        \n",
    "        # указатель цели (без CollisionShape, только отображение(VisualShape))\n",
    "        targetPosition = [0,0,self.z_target]\n",
    "        targetOrientation=[0,0,0,1]\n",
    "        targetVisualShapeId = pb.createVisualShape(pb.GEOM_BOX,halfExtents=[1,0.025,0.025], rgbaColor=[0,0,0,1])\n",
    "        self.pb_targetId = pb.createMultiBody(0,-1, targetVisualShapeId, targetPosition, targetOrientation)\n",
    "\n",
    "        # гравитация\n",
    "        pb.setGravity(0,0,-10)\n",
    "\n",
    "        # ограничим движение шара только по вертикальной оси\n",
    "        pb.createConstraint(self.pb_floorId, -1, self.pb_ballId, -1, pb.JOINT_PRISMATIC, [0,0,1], [0,0,0], [0,0,0])\n",
    "\n",
    "        # установим действующую силу на шар, чтобы компенсировать гравитацию\n",
    "        self.pb_force = 10 * PB_BallMass\n",
    "        pb.applyExternalForce(self.pb_ballId, -1, [0,0,self.pb_force], [0,0,0], pb.LINK_FRAME)\n",
    "                \n",
    "        # return values\n",
    "        observation = self.getObservation()\n",
    "        reward, done = self.getReward()\n",
    "        info = self.getInfo()\n",
    "        return [observation, reward, done, info]\n",
    "\n",
    "    # Наблюдения (возвращаются нормализованными)\n",
    "    def getObservation(self):\n",
    "        # расстояние до цели\n",
    "        d_target =  0.5 + (self.pb_z - self.z_target)/(2*PB_HEIGHT)\n",
    "        # действующая сила\n",
    "        force = (self.pb_force-MIN_FORCE)/(MAX_FORCE-MIN_FORCE)\n",
    "        # текущая высота шара\n",
    "        z = (self.pb_z-PB_BallRadius)/PB_HEIGHT\n",
    "        # текущая скорость\n",
    "        z_velocity = (self.pb_velocity-MIN_VEL)/(MAX_VEL-MIN_VEL)\n",
    "        state = [d_target, force, z_velocity]\n",
    "        return state\n",
    "\n",
    "    # вычисление награды за действие\n",
    "    def getReward(self):\n",
    "        done = False\n",
    "        z_reward = 0\n",
    "        # Факт достижения цели, после чего ждем STEPS_AFTER_TARGET шагов и завершем игру.\n",
    "        if (TARGET_DELTA >= math.fabs(self.z_target - self.pb_z)):\n",
    "            self.target_area = 1\n",
    "            z_reward = 1\n",
    "        # Выход за пределы зоны\n",
    "        if (self.pb_z > (PB_HEIGHT + PB_BallRadius) or self.pb_z < PB_BallRadius):\n",
    "            done = True\n",
    "        # Завершение игры после достижения цели\n",
    "        if (self.target_area > 0):\n",
    "            self.steps_after_target += 1\n",
    "            if (self.steps_after_target>=STEPS_AFTER_TARGET):\n",
    "                done = True\n",
    "        # Завершение игры по таймауту\n",
    "        if (self.steps >= MAX_STEPS):\n",
    "            done = True\n",
    "\n",
    "        return [z_reward, done]\n",
    "    \n",
    "    # Дополнительная информация для сбора статистики\n",
    "    def getInfo(self):\n",
    "        game_time = time.time() - self.start_time\n",
    "        if game_time:\n",
    "            fps = round(self.steps/game_time)\n",
    "        return {'step': self.steps, 'fps': fps}\n",
    "\n",
    "    # Запуск шага симуляции согласно переданному действию\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        if action == 0:\n",
    "            # 0 - увеличение приложеной силы\n",
    "            self.pb_force -= FORCE_DELTA\n",
    "            if self.pb_force < MIN_FORCE:\n",
    "                self.pb_force = MIN_FORCE\n",
    "        else:\n",
    "            # 1 - уменьшение приложенной силы\n",
    "            self.pb_force += FORCE_DELTA\n",
    "            if self.pb_force > MAX_FORCE:\n",
    "                self.pb_force = MAX_FORCE\n",
    "        \n",
    "        # изменим текущую сил и запустим шаг симуляции\n",
    "        pb.applyExternalForce(self.pb_ballId, -1, [0,0,self.pb_force], [0,0,0], pb.LINK_FRAME)\n",
    "        pb.stepSimulation()\n",
    "        \n",
    "        # обновим парамтры состояния окружения (положение и скорость шара)\n",
    "        curPos, curOrient = pb.getBasePositionAndOrientation(self.pb_ballId)\n",
    "        lin_vel, ang_vel= pb.getBaseVelocity(self.pb_ballId)\n",
    "        self.pb_z = curPos[2]\n",
    "        self.pb_velocity = lin_vel[2]\n",
    "        \n",
    "        # вернем наблюдения, награду, факт окончания игры и доп.информацию\n",
    "        observation = self.getObservation()\n",
    "        reward, done = self.getReward()\n",
    "        info = self.getInfo()\n",
    "        return [observation, reward, done, info]\n",
    "    \n",
    "    # Текущее изображение с камеры\n",
    "    def render(self):\n",
    "        camTargetPos = [0,0,5] # расположение цели (фокуса) камеры\n",
    "        camDistance = 10       # дистанция камеры от цели\n",
    "        yaw = 0                # угол рыскания относительно цели\n",
    "        pitch = 0              # наклон камеры относительно цели\n",
    "        roll=0                 # угол крена камеры относительно цели\n",
    "        upAxisIndex = 2        # ось вертикали камеры (z)\n",
    "\n",
    "        fov = 60               # угол зрения камеры\n",
    "        nearPlane = 0.01       # расстояние до ближней плоскости отсечения\n",
    "        farPlane = 20          # расстояние до дальной плоскости отсечения\n",
    "        pixelWidth = 320       # ширина изображения\n",
    "        pixelHeight = 200      # высота изображения\n",
    "        aspect = pixelWidth/pixelHeight;  # соотношение сторон изображения\n",
    "       \n",
    "        # видовая матрица\n",
    "        viewMatrix = pb.computeViewMatrixFromYawPitchRoll(camTargetPos, camDistance, yaw, pitch, roll, upAxisIndex)\n",
    "        # проекционная матрица\n",
    "        projectionMatrix = pb.computeProjectionMatrixFOV(fov, aspect, nearPlane, farPlane);\n",
    "        # рендеринг изображения с камеры\n",
    "        img_arr = pb.getCameraImage(pixelWidth, pixelHeight, viewMatrix, projectionMatrix, shadow=0, lightDirection=[0,1,1],renderer=pb.ER_TINY_RENDERER)\n",
    "        w=img_arr[0] #width of the image, in pixels\n",
    "        h=img_arr[1] #height of the image, in pixels\n",
    "        rgb=img_arr[2] #color data RGB\n",
    "        dep=img_arr[3] #depth data\n",
    "        \n",
    "        # вернем rgb матрицу\n",
    "        return rgb"
   ]
  },
  {
   "source": [
    "## Память для обучающих примеров"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_CAPACITY = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.samples = []   # хранятся кортежи типа ( s, a, r, s_ )\n",
    "\n",
    "    def add(self, sample):\n",
    "        self.samples.append(sample)\n",
    "        if len(self.samples) > MEMORY_CAPACITY:\n",
    "            self.samples.pop(0)\n",
    "\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)"
   ]
  },
  {
   "source": [
    "## Нейронная сеть"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_SIZE = 512       # размер слоя\n",
    "STATE_CNT  = 3         # количество входных пераметров (расстояние до цели + действующая сила + скорость)\n",
    "ACTION_CNT = 2         # количесво выходов (награда за уменьшение и увеличение силы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self):\n",
    "        self.model = self._QNetwork()\n",
    "        \n",
    "    def _QNetwork(self):\n",
    "        # Создадим сеть используя Keras\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=LAYER_SIZE, activation='relu', input_dim=STATE_CNT))\n",
    "        model.add(Dense(units=LAYER_SIZE, activation='relu'))\n",
    "        model.add(Dense(units=ACTION_CNT, activation='linear'))\n",
    "        opt = RMSprop(lr=0.00025)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "        return model\n",
    "    \n",
    "    # обучение по одному пакету обучающих примеров \n",
    "    def train(self, x, y, batch_size=32, epoch=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=batch_size, epochs=epoch, verbose=verbose)\n",
    "\n",
    "    # предсказания сети по списку начальных состояний\n",
    "    def predict(self, s):\n",
    "        return self.model.predict(s)\n",
    "\n",
    "    #  предсказания сети по одном начальному состоянию\n",
    "    def predictOne(self, s):\n",
    "        s = numpy.array(s)\n",
    "        predictions = self.predict(s.reshape(1, STATE_CNT)).flatten()\n",
    "        return predictions"
   ]
  },
  {
   "source": [
    "## Агент"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.98        # фактор дисконтирования\n",
    "MAX_EPSILON = 0.5   # максимальная вероятность выбора случайного действия\n",
    "MIN_EPSILON = 0.1   # минимальная вероятность выбора случайного действия\n",
    "LAMBDA = 0.001      # параметр определяющий скорость уменьшения вероятности выбора случайного действия\n",
    "BATCH_SIZE = 32     # размер обучающего пакета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.brain = Brain()                     # Нейронная сеть для обучения\n",
    "        self.memory = Memory()                   # Хранилище обучающих примеров\n",
    "        self.epsilon = MAX_EPSILON               # Определяет вероятность выбора случайного действия\n",
    "\n",
    "    # выбор действия\n",
    "    def act(self, s):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, ACTION_CNT - 1)        # выбираем случайное действие\n",
    "        else:\n",
    "            return numpy.argmax(self.brain.predictOne(s))   # выбираем оптимальное действие\n",
    "        \n",
    "    # изменение состояния агента\n",
    "    def observe(self, sample, game_num):  # sample = (s, a, r, s_)\n",
    "        self.memory.add(sample)\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON-MIN_EPSILON)*math.exp(-LAMBDA*game_num)\n",
    "\n",
    "    # обучение по случайному пакету обучающих примеров (batch)\n",
    "    def train(self):\n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        batchLen = len(batch)  \n",
    "        if batchLen<BATCH_SIZE: # будем обучаться только если есть достаточное количество примеров в памяти\n",
    "            return\n",
    "\n",
    "        # начальные состояния из пакета\n",
    "        states = numpy.array([ o[0] for o in batch ])\n",
    "        # начальные состояния из пакета\n",
    "        states_ = numpy.array([ o[3] for o in batch ])\n",
    "\n",
    "        # выгоды для начальных состояний\n",
    "        p = agent.brain.predict(states)\n",
    "        # выгоды для конечных состояний\n",
    "        p_ = agent.brain.predict(states_)\n",
    "\n",
    "        # сформируем пустой обучающий пакет\n",
    "        x = numpy.zeros((batchLen, STATE_CNT))\n",
    "        y = numpy.zeros((batchLen, ACTION_CNT))\n",
    "\n",
    "        # заполним пакет\n",
    "        for i in range(batchLen):\n",
    "            o = batch[i]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "\n",
    "            t = p[i] # выгоды действий для начального состояния\n",
    "            # обновим выгоду только для совершенного действия, для неиспользованных действий выгоды останутся прежними\n",
    "            t[a] = r + GAMMA * numpy.amax(p_[i]) # вычислим новую выгоду действия используя награду и максимальную выгоду конечного состояния\n",
    "            \n",
    "            # сохраним значения в batch\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "\n",
    "        # обучим сеть по данному пакету\n",
    "        self.brain.train(x, y)"
   ]
  },
  {
   "source": [
    "## Статистика"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stats():\n",
    "    def __init__(self):\n",
    "        self.stats={\"game_num\": [],\"rewards\": [], \"success_steps\": [], \"fps\": [], \"steps\":[], \"epsilon\":[]}\n",
    "\n",
    "    def save_stat(self, R, info, epsilon, game_num):\n",
    "        self.stats[\"rewards\"].append(R)\n",
    "        self.stats[\"success_steps\"].append(R/STEPS_AFTER_TARGET)\n",
    "        self.stats[\"game_num\"].append(game_num)\n",
    "        self.stats[\"epsilon\"].append(epsilon)\n",
    "        self.stats[\"steps\"].append(info[\"step\"])\n",
    "        self.stats[\"fps\"].append(info[\"fps\"])\n",
    "    def show_stat(self):\n",
    "        # отобраим процент удачных шагов за опыт\n",
    "        plt.plot(self.stats[\"game_num\"], self.stats[\"success_steps\"], \"b.\")\n",
    "        # отобразим сглаженный график\n",
    "        x, y = self.fit_data(self.stats[\"game_num\"],  self.stats[\"success_steps\"])\n",
    "        plt.plot(x, y, \"r-\")\n",
    "        # второй вариант сглаживания    \n",
    "        # plt.plot(numpy.linspace(self.stats[\"game_num\"][0], self.stats[\"game_num\"][-1],50), numpy.average(numpy.array_split(self.stats[\"success_steps\"][:-1], 50),1), \"g-\")\n",
    "        plt.show()\n",
    "    #  Полиномиальное сглаживание\n",
    "    def fit_data(self, x, y):\n",
    "        z = numpy.polyfit(x, y, 3)\n",
    "        f = numpy.poly1d(z)\n",
    "        # новые данные размерностью 50\n",
    "        x_new = numpy.linspace(x[0], x[-1], 50)\n",
    "        y_new = f(x_new)\n",
    "        return [x_new, y_new]"
   ]
  },
  {
   "source": [
    "## MAIN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "MAX_GAMES = 50000   # максимальное количество игр\n",
    "RENDER_PERIOD = 0 # период генерации видео с опытом (0 для отключения)\n",
    "\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "stats = Stats()\n",
    "\n",
    "for game_num in range(MAX_GAMES):\n",
    "    print (\"Game %d:\" % game_num)\n",
    "    render_imgs = []\n",
    "    observation, r, done, info = env.reset()\n",
    "    s = observation\n",
    "    R = r\n",
    "    \n",
    "    if RENDER_PERIOD and (game_num % RENDER_PERIOD == 0):\n",
    "        plt.subplots()\n",
    "    \n",
    "    while True:\n",
    "        # возьмем оптимальное действие на основе текущего состояния\n",
    "        a = agent.act(s)\n",
    "        # запустим шаг симуляции\n",
    "        observation, r, done, info = env.step(a)\n",
    "        s_ = observation # новое состояние\n",
    "        # сохраним состояние агента\n",
    "        agent.observe((s, a, r, s_), game_num)\n",
    "        # обучим сеть по случайносу batch-у\n",
    "        agent.train()\n",
    "        \n",
    "        s = s_\n",
    "        R += r\n",
    "        \n",
    "        # сохраним изображение, если необходимо\n",
    "        if RENDER_PERIOD and game_num % RENDER_PERIOD == 0:\n",
    "            rgb = env.render()\n",
    "            render_imgs.append([plt.imshow(rgb, animated=True)])\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        #time.sleep(1./130)\n",
    "\n",
    "    print(\"Total reward:\", R, \" FPS:\", info['fps'])\n",
    "    \n",
    "    # сохраним статистику\n",
    "    stats.save_stat(R, info, agent.epsilon, game_num)\n",
    "    \n",
    "    # сформируем анимацию игры и графики статистики обучения\n",
    "    if len(render_imgs):\n",
    "        render_start = time.time()\n",
    "        # plt.rcParams['animation.ffmpeg_path'] = 'C:\\FFmpeg\\bin\\ffmpeg.exe'\n",
    "        ani = animation.ArtistAnimation(plt.gcf(), render_imgs, interval=10, blit=True,repeat_delay=1000)\n",
    "        plt.close()\n",
    "        anima = ani.to_html5_video()\n",
    "        display(HTML(anima))\n",
    "        # статистика\n",
    "        if game_num != 0:\n",
    "            plt.subplots(figsize=(10,4))\n",
    "            stats.show_stat()\n",
    "            plt.close()\n",
    "        render_stop = time.time()\n",
    "        print (\"render time: %f sec.\\n---\\n\" % (render_stop - render_start))"
   ]
  }
 ]
}